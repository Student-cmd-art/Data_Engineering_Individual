{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of a RAG System for Data Science Knowledge Management\n",
        "\n",
        "\n",
        "<h1> Domain Understanding</h1>\n",
        "Kaggle is a highly popular platform in the data science community. As of 2021, it had over 8 million registered users.\n",
        "\n",
        "Kaggle has gained popularity by running competitions that range from fun brain exercises to commercial contests that award monetary prizes and rank participants. These competitions often involve real-world problems and offer substantial prizes, attracting data scientists from around the world to participate and learn.\n",
        "\n",
        "Moreover, Kaggle is not just a competition platform. It's a comprehensive data science ecosystem that includes public datasets, collaborative notebooks, and a robust discussion forum. This makes it a go-to resource for data scientists to learn new skills, collaborate on projects, and stay updated with the latest trends in the field.\n",
        "\n",
        "<h1> Problem Description </h1>\n",
        "\n",
        "The primary objective of this use case is to utilize the capabilities of Large Language Models (LLMs), under the Retrieval-Augmented Generation (RAG) architecture, to enhance the understanding and utilization of the Kaggle platform and Python programming language within an enterprise setting.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Kaggle Solution Summarization: The LLM will summarize Kaggle solution write-ups, providing concise and understandable summaries of complex data science solutions.\n",
        "\n",
        "2. Kaggle Competition Concept Explanation: The LLM will explain or teach concepts from Kaggle competition solution write-ups, aiding in the understanding and learning of advanced data science techniques and methodologies.\n",
        "\n",
        "3. Kaggle Platform Query Resolution: The LLM will answer common questions about the Kaggle platform, assisting users in navigating and utilizing the platform effectively.\n",
        "\n",
        "\n",
        "<h1> Project Process </h1>\n",
        "\n",
        "\n",
        "1. **Data Acquisition:**\n",
        "\n",
        " Initially, the data is gathered and web scraped from Kaggle’s documentation website called “How to use Kaggle”. This source includes information about Kaggle competitions, datasets, notebook discussions, documentation, etc. This data will serve as the knowledge source for the large language model.\n",
        "\n",
        "2. **Data Preprocessing:**\n",
        "\n",
        "  The data is then broken down into smaller chunks and converted into a vector database for efficient querying. For the purpose of a vector database, the ChromaDB will be utilised which is used to store the vectors and query them.is used to store the vectors and query them. The embedding function used is  HuggingFace InstructEmbeddings, an embedding model to convert the chunks into vectors.\n",
        "\n",
        "3. **Query for Relevant Data & Craft Response:**\n",
        "  Here, the user query and context will be used to generate a response using the Gemma LLM. A prompt is used to combine the question and the retrieved documents from the vector database.\n",
        "\n",
        "  The Gemma 7B Instruct model will be employed for this task. However, given its substantial size, it will be presented through the Inference API on Hugging Face for efficiency."
      ],
      "metadata": {
        "id": "ouEYuLr98V2G"
      }
    }
  ]
}