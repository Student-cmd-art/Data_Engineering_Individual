{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of a RAG System for Data Science Knowledge Management\n",
        "\n",
        "\n",
        "<h1> Domain Understanding</h1>\n",
        "Kaggle is a highly popular platform in the data science community. As of 2021, it had over 8 million registered users.\n",
        "\n",
        "Kaggle has gained popularity by running competitions that range from fun brain exercises to commercial contests that award monetary prizes and rank participants. These competitions often involve real-world problems and offer substantial prizes, attracting data scientists from around the world to participate and learn.\n",
        "\n",
        "Moreover, Kaggle is not just a competition platform. It's a comprehensive data science ecosystem that includes public datasets, collaborative notebooks, and a robust discussion forum. This makes it a go-to resource for data scientists to learn new skills, collaborate on projects, and stay updated with the latest trends in the field.\n",
        "\n",
        "<h1> Problem Description </h1>\n",
        "\n",
        "The primary objective of this use case is to utilize the capabilities of Large Language Models (LLMs), under the Retrieval-Augmented Generation (RAG) architecture, to enhance the understanding and utilization of the Kaggle platform and Python programming language within an enterprise setting.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Kaggle Solution Summarization: The LLM will summarize Kaggle solution write-ups, providing concise and understandable summaries of complex data science solutions.\n",
        "\n",
        "2. Kaggle Competition Concept Explanation: The LLM will explain or teach concepts from Kaggle competition solution write-ups, aiding in the understanding and learning of advanced data science techniques and methodologies.\n",
        "\n",
        "3. Kaggle Platform Query Resolution: The LLM will answer common questions about the Kaggle platform, assisting users in navigating and utilizing the platform effectively.\n",
        "\n",
        "\n",
        "<h1> Project Process </h1>\n",
        "\n",
        "\n",
        "1. **Data Acquisition:**\n",
        "\n",
        " Initially, the data is gathered and web scraped from Kaggle’s documentation website called “How to use Kaggle”. This source includes information about Kaggle competitions, datasets, notebook discussions, documentation, etc. This data will serve as the knowledge source for the large language model.\n",
        "\n",
        "2. **Data Preprocessing:**\n",
        "\n",
        "  The data is then broken down into smaller chunks and converted into a vector database for efficient querying. For the purpose of a vector database, the ChromaDB will be utilised which is used to store the vectors and query them.is used to store the vectors and query them. The embedding function used is  HuggingFace InstructEmbeddings, an embedding model to convert the chunks into vectors.\n",
        "\n",
        "3. **Query for Relevant Data & Craft Response:**\n",
        "  Here, the user query and context will be used to generate a response using the Gemma LLM. A prompt is used to combine the question and the retrieved documents from the vector database.\n",
        "\n",
        "  The Gemma 7B Instruct model will be employed for this task. However, given its substantial size, it will be presented through the Inference API on Hugging Face for efficiency."
      ],
      "metadata": {
        "id": "ouEYuLr98V2G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ssy1NZvA1hfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Libraries"
      ],
      "metadata": {
        "id": "V7jpHb2y8V2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "!pip install sentence-transformers==2.2.2  # Install a specific version of sentence-transformers that is compatible with the setup\n",
        "!pip install unstructured  # Install the unstructured package, which provides functionalities related to handling unstructured data, such as text processing and NLP\n",
        "!pip install langchain  # Install the langchain package for language processing\n",
        "!pip install chromadb  # Install the Chroma database for efficient querying\n",
        "!pip install InstructorEmbedding  # Install the InstructorEmbedding package for converting chunks into vectors\n"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-30T07:55:50.18903Z",
          "iopub.execute_input": "2024-03-30T07:55:50.189325Z",
          "iopub.status.idle": "2024-03-30T07:58:00.293886Z",
          "shell.execute_reply.started": "2024-03-30T07:55:50.189297Z",
          "shell.execute_reply": "2024-03-30T07:58:00.292737Z"
        },
        "trusted": true,
        "id": "yGMSe9ck8V2H",
        "outputId": "286e510a-4989-4dfe-863d-15c50e96f33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m61.4/86.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=0535ac0dfbd17805d8cc3039e834ba8a7ceb870240557cc94b80d5eaf0c62cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.13.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.10.0)\n",
            "Collecting unstructured-client<=0.18.0 (from unstructured)\n",
            "  Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (3.3.2)\n",
            "Collecting dataclasses-json-speakeasy>=0.5.11 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (3.6)\n",
            "Collecting jsonpath-python>=1.0.6 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow>=3.19.0 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=1.0.0 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (1.16.0)\n",
            "Collecting typing-inspect>=0.9.0 (from unstructured-client<=0.18.0->unstructured)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client<=0.18.0->unstructured) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=5114d9dc15c634966898c3b378333a9194227f850a0980b7716a9b95157ea322\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, mypy-extensions, marshmallow, langdetect, jsonpath-python, emoji, backoff, typing-inspect, dataclasses-json-speakeasy, dataclasses-json, unstructured-client, unstructured\n",
            "Successfully installed backoff-2.2.1 dataclasses-json-0.6.4 dataclasses-json-speakeasy-0.5.11 emoji-2.11.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 marshmallow-3.21.1 mypy-extensions-1.0.0 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.8.1 typing-inspect-0.9.0 unstructured-0.13.2 unstructured-client-0.18.0\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.14-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.30 (from langchain)\n",
            "  Downloading langchain_community-0.0.31-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.37 (from langchain)\n",
            "  Downloading langchain_core-0.1.40-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.37->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.14 langchain-community-0.0.31 langchain-core-0.1.40 langchain-text-splitters-0.0.1 langsmith-0.1.40 orjson-3.10.0 packaging-23.2\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=a2135808e154bdaf04f7b777af76d6d9d081750a8fe176440a9e9f075cd4ead0\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed asgiref-3.8.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting InstructorEmbedding\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: InstructorEmbedding\n",
            "Successfully installed InstructorEmbedding-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "rcWDU2RT8V2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Python libraries for operating system and file operations\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Python library for making HTTP requests\n",
        "import requests\n",
        "\n",
        "# Python library for parsing HTML and XML documents\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Standard Python library for regular expressions\n",
        "import re\n",
        "\n",
        "# Hypothetical module that provides embeddings for the instructor\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "\n",
        "# Module that provides embeddings using Hugging Face's models\n",
        "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "# Module that loads documents from a directory\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "# Module that splits text into smaller chunks recursively\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Module that provides a template for chat prompts\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Module that provides a vector database for efficient querying\n",
        "from langchain.vectorstores import Chroma\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:58:00.296032Z",
          "iopub.execute_input": "2024-03-30T07:58:00.296339Z",
          "iopub.status.idle": "2024-03-30T07:58:08.887314Z",
          "shell.execute_reply.started": "2024-03-30T07:58:00.296312Z",
          "shell.execute_reply": "2024-03-30T07:58:08.8865Z"
        },
        "trusted": true,
        "id": "Ag9Ya3ia8V2H",
        "outputId": "907c97ca-21f3-4e5f-dfc2-4323e25fcf93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition - Web Scraping\n"
      ],
      "metadata": {
        "id": "d-ZQstmR8V2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove special characters, escape sequences, and links from text\n",
        "def special_character_removal(text):\n",
        "    \"\"\"\n",
        "    Removes escape characters, symbols, and links from the input text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text with special characters, escape sequences, and links removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove escape characters, symbols, and links\n",
        "    cleaned_text = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', text)  # Remove unicode escape sequences\n",
        "    cleaned_text = re.sub(r'\\\\n', '', cleaned_text)  # Remove newline escape sequences\n",
        "    cleaned_text = re.sub(r'https?://\\S+', '', cleaned_text)  # Remove URLs\n",
        "\n",
        "    # Extract sentences using regex\n",
        "    sentences_final = re.findall(r'([^.!?]+(?:[.!?]+|$))', cleaned_text)\n",
        "\n",
        "    # Recombines the extracted sentences into a single string, separated by spaces.\n",
        "    combined_text_final = ' '.join(sentences_final)\n",
        "\n",
        "    # Returns the combined text with all extracted sentences.\n",
        "    return combined_text_final\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:08.968953Z",
          "iopub.execute_input": "2024-03-30T07:59:08.969323Z",
          "iopub.status.idle": "2024-03-30T07:59:08.975049Z",
          "shell.execute_reply.started": "2024-03-30T07:59:08.969294Z",
          "shell.execute_reply": "2024-03-30T07:59:08.974105Z"
        },
        "trusted": true,
        "id": "3KzFGlfr8V2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of endpoints from the Kaggle documentation web page\n",
        "documentation_endpoints = [\"competitions\", \"datasets\", \"notebooks\", \"api\"]\n",
        "\n",
        "documentation_url = \"https://www.kaggle.com/docs/\"\n",
        "OUTPUT_DIRECTORY = \"/content/DE_CW/\"\n",
        "\n",
        "# Iterate through each endpoint of the documentation\n",
        "for endpoint in documentation_endpoints:\n",
        "\n",
        "    # Complete URL for the documentation page\n",
        "    endpoint_url = f\"{documentation_url}{endpoint}\"\n",
        "    # HTTP GET request for the documentation page\n",
        "    response = requests.get(endpoint_url)\n",
        "\n",
        "    if response.status_code == 200:  # If the request is successful\n",
        "        response = response.content  # Get binary data from the response\n",
        "        soup = BeautifulSoup(response, \"html.parser\")  # Parse HTML content using BeautifulSoup\n",
        "\n",
        "        # Find the main content component of the page\n",
        "        page_components = soup.find(class_=\"kaggle-component\")\n",
        "        # Get HTML content as a string\n",
        "        html_content_str = page_components.prettify()\n",
        "\n",
        "        # Clean the HTML content by removing special characters, escape sequences, and links\n",
        "        cleaned_content = special_character_removal(html_content_str)\n",
        "\n",
        "        # Save the cleaned content to a text file\n",
        "        with open(f'{OUTPUT_DIRECTORY}{endpoint}.txt', 'w') as file:\n",
        "            file.writelines(cleaned_content)\n",
        "            print(f\"{endpoint} saved to {OUTPUT_DIRECTORY}\")\n",
        "            file.close()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:09.905114Z",
          "iopub.execute_input": "2024-03-30T07:59:09.90548Z",
          "iopub.status.idle": "2024-03-30T07:59:10.963367Z",
          "shell.execute_reply.started": "2024-03-30T07:59:09.90545Z",
          "shell.execute_reply": "2024-03-30T07:59:10.962451Z"
        },
        "trusted": true,
        "id": "kK8DVAY88V2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9af66e-f115-4a52-bfe5-1701674d73c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "competitions saved to /content/DE_CW/\n",
            "datasets saved to /content/DE_CW/\n",
            "notebooks saved to /content/DE_CW/\n",
            "api saved to /content/DE_CW/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preparation\n",
        "The first part of the process involves loading the data. The load_txt_documents(OUTPUT_DIRECTORY) function is used for this purpose. It utilizes the DirectoryLoader() class to load all .txt files from the specified directory (OUTPUT_DIRECTORY).\n",
        "\n",
        "The glob parameter is set to \"*.txt\", which means it will look for all files with the .txt extension. Once the documents are loaded, the function prints the number of documents found and returns a list of these documents."
      ],
      "metadata": {
        "id": "UNWW9JZa8V2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load .txt data from a folder\n",
        "def load_txt_documents(OUTPUT_DIRECTORY):\n",
        "    \"\"\"\n",
        "    Load .txt documents from the specified directory.\n",
        "\n",
        "    Args:\n",
        "        OUTPUT_DIRECTORY (str): The path to the directory containing the .txt documents.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of loaded .txt documents.\n",
        "    \"\"\"\n",
        "    # Use DirectoryLoader from langchain package to load documents\n",
        "    loader = DirectoryLoader(OUTPUT_DIRECTORY, glob=\"*.txt\")\n",
        "    # Load the documents\n",
        "    documents = loader.load()\n",
        "    # Print the number of documents found\n",
        "    print(f\"{len(documents)} document(s) found.\")\n",
        "    # Return the loaded documents\n",
        "    return documents"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:12.809535Z",
          "iopub.execute_input": "2024-03-30T07:59:12.810252Z",
          "iopub.status.idle": "2024-03-30T07:59:12.814848Z",
          "shell.execute_reply.started": "2024-03-30T07:59:12.810219Z",
          "shell.execute_reply": "2024-03-30T07:59:12.81387Z"
        },
        "trusted": true,
        "id": "SZdNuobq8V2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load .txt data from folder\n",
        "documents = load_txt_documents(OUTPUT_DIRECTORY)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:13.66863Z",
          "iopub.execute_input": "2024-03-30T07:59:13.669687Z",
          "iopub.status.idle": "2024-03-30T07:59:18.814978Z",
          "shell.execute_reply.started": "2024-03-30T07:59:13.669645Z",
          "shell.execute_reply": "2024-03-30T07:59:18.814053Z"
        },
        "trusted": true,
        "id": "p8CSQfjh8V2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ff2950-975f-45e4-b332-11f265ffd314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 document(s) found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LWBZ7w5sJpc",
        "outputId": "cf758c68-f492-4b84-f060-60757b7509fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='<script class=\"kaggle-component\" nonce=\"FfUlfah2rDrp3nOitcrFdQ==\"> var Kaggle=window. Kaggle||{};Kaggle. State=Kaggle. State||[];Kaggle. State. push({\"title\":\"Datasets\",\"subtitle\":\"Explore, analyze, and share quality data\",\"imageUrl\":\"\",\"mimeType\":\"text/html\",\"pageContent\":\"hr width=100% align=left! --Types of Datasets--h3 id=types-of-datasetsTypes of Datasets/h3pKaggle supports a variety of dataset publication formats, but we strongly encourage dataset publishers to share their data in an accessible, non-proprietary format if possible. Not only are open, accessible data formats better supported on the platform, they are also easier to work with for more people regardless of their tools. /ppThis page describes the file formats that we recommend using when sharing data on Kaggle Datasets. Plus, learn why and how to make less well-supported file types as accessible as possible to the data science community. /p    h4 id=supported-file-typesSupported File Types/h4        h5 id=csvsCSVs/h5        pThe simplest and best-supported file type available on Kaggle is the Comma-Separated List, or CSV, for tabular data. CSVs uploaded to Kaggle should have a header row consisting of human-readable field names. A CSV representation of a shopping list with a header row, for example, looks like this:/p        pid,type,quantity/p        p0,bananas,12/p        p1,apples,7/p        pCSVs are the most common of the file formats available on Kaggle and are the best choice for tabular data.\\n\\n/p        pOn the Data tab of a dataset, a preview of the files contents is visible in the data explorer. This makes it significantly easier to understand the contents of a dataset, as it eliminates the need to open the data in a Notebook or download it locally. /p        pCSV files will also have associated column descriptions and column metadata. The column descriptions allows you to assign descriptions to individual columns of the dataset, making it easier for users to understand what each column means. Column metrics, meanwhile, present high-level metrics about individual columns in a graphic format. /p        pa href= Complete Pokemon Dataset/a is an example of a great CSV-type Dataset. /p        h5 id=jsonJSON/h5        pWhile CSV is the most common file format for flat data, JSON is the most common file format for tree-like data that potentially has multiple layers, like the branches on a tree:/p        p{[{id: 0, type: bananas, quantity: 12}, {id: 1, type: apples, quantity: 7}]}/p        pFor JSON files, the Data tab preview will present an interactive tree with the nodes in the JSON file attached. You can click on individual keys to open and collapse sections of the tree, exploring the structure of the dataset as you go along. JSON files do not support column descriptions or metrics. /p        pYou can filter the Datasets listing by File Type to show a href= datasets containing JSON files/a.\\n\\n/p        h5 id=sqliteSQLite/h5        pKaggle supports database files using the lightweight SQLite format. SQLite databases consist of multiple tables, each of which contains data in tabular format. These tables support large datasets better than CSV files do, but are otherwise similar in practice. /p        pThe Data tab represents each table in a database separately. Like CSV files, SQLite tables will be fully populated by Column Metadata and Column Metrics sections. /p        pa href= Soccer Database/a is an example of a great SQLite-type Dataset. /p        h5 id=archivesArchives/h5        pAlthough not technically a file format per se, Kaggle also has first-class support for files compressed using the ZIP file format as well as other common archive formats like 7z. /p        pCompressed files take up less space on disk than uncompressed ones, making them significantly faster to upload to Kaggle and allowing you to upload datasets that would otherwise exceed the Dataset size limitations. /p        pArchives are uncompressed on our side so that their contents are accessible in Notebooks without requiring users to unzip them. Archives do not currently populate previews for individual file contents, but you can still browse the contents by file name. /p        pAs a result, we recommend that you only upload your dataset as an archive if the dataset is large enough, is made up of many smaller files, or is organized into subfolders.\\n\\nFor instance, ZIPs and other archive formats are a great choice for making image datasets available on Kaggle. /p        pa href= X-Ray Images (Pneumonia)/a is an example of a dataset made of archived images. /p        h5 id=bigqueryBigQuery/h5        pKaggle also supports special BigQuery Datasets. BigQuery is a big data SQL store invented by Google. Many massive public datasets, like all the code in GitHub and the complete history of the Bitcoin blockchain, are available publically through the Google BigQuery Public Datasets initiative. Some of these are in turn also available as Kaggle Datasets! /p        pBigQuery Datasets are special in many ways. Because they are multi-terabyte datasets hosted on Googles servers they cannot be uploaded or downloaded. Within Notebooks, instead of loading the files from disk, you interact with the dataset by writing SQL fetch queries within either the Google BigQuery Python library or Kaggles bq_helper library. And, due to the large size of the datasets involved, there is a quota of 5 TB of data scanned per user per 30-days. /p        pSome resources for understanding how to use BigQuery:/p        ul style=list-style-type:disc        \\\\tli        \\\\t\\\\tpa href= Started with Big Query/a/p        \\\\t/li        \\\\tli        \\\\t\\\\tpa href= Queries: Exploring the Bigquery API/a/p        \\\\t/li        /ul        pa href= Names Data/a is an example of a BigQuery-type Dataset.\\n\\nHere are some helpful Notebooks for learning more about BigQuery: a href= Scavenger Hunt Handbook/a, a href= Started with BigQuery/a, and a href= Queries: Exploring the BigQuery API/a. /p        h5 id=other-file-formatsOther File Formats/h5        pThe file formats listed in the section above are the ones best supported and most common on the Kaggle format. This doesnt mean that other types of files cant be uploaded; any file you can think of can be uploaded. Other formats are just less well-supported: they may not have previews or any of the other data explorer components available. They will also likely be less familiar with Kaggle users, and hence, less accessible. /p        pIf you can convert your file into one of the formats above (the simpler the better), we highly recommend doing so. For example, Excel spreadsheets are a proprietary format that should be uploaded as CSV files instead. Your users will thank you! /p        pHowever, there are nevertheless use cases for alternative data formats. We do encourage uploads in speciality data formats like NPZ, image file formats like PNG, and complex hierarchical data formats like HDF5. But, when doing so, we suggest also uploading a Notebook discussing what and where the files are, how to work with them, and demonstrating how to get started with the dataset. Reproducible code samples can go a long way towards making your data files accessible to the data science world! /phr width=100% align=left!\\n\\n--Searching for Datasets--h3 id=searching-for-datasetsSearching for Datasets/h3pDatasets is not just a simple data repository. Each dataset is a community where you can discuss data, discover public code and techniques, and create your own projects in Notebooks. You can find many different interesting datasets of all shapes and sizes if you take the time to look around and find them! /ppThe latest and greatest from Datasets is surfaced on Kaggle in several different places. /p    h4 id=newsfeedNewsfeed/h4    pWhen youre logged into your Kaggle account, the a href= homepage/a provides a live newsfeed of what people are doing on the platform. New Datasets uploaded by people you follow and hot Datasets with lots of activity will show up here. By browsing down the page you can check out all the latest updates from your fellow Kagglers. /p    pYou can tweak your news feed to your liking by following other Kagglers. To follow someone, go to their profile page and click on Follow User. Content posted and upvotes made by users you have followed will show up more prominently. /p    pThe same is true of other users who choose to follow you. Post high-quality content and you will soon find other users following along with what you are doing! /p    h4 id=datasets-listingDatasets Listing/h4    pA more structured way of accessing datasets is accessible from the Datasets tab in the main menu bar.\\n\\n/p    pDatasets are grouped by different categories: Trending Datasets, Popular Datasets, Recently Viewed Datasets and a few other rotating categories. /p    pAt the bottom of this page, you can click on the Explore all public datasets button to get a list view of all datasets. The list is sorted by Hotness by default. Hotness is what it sounds like: a way of measuring the interestingness and recency of datasets on the platform. Datasets which score highly in Hotness, and thus appear highly in this list, are usually either recently released Datasets that have been marked Reviewed and are scoring highly in engagement, or all-time greats that have been consistently popular on the platform for a long time. /p    pOther methods of sorting are by Most Votes, New, Updated and Usability. /p    pOther filtering options, available from the navigation bar, are Sizes (Small, Medium, or Large), File types (CSV, SQLite, JSON, BigQuery), Licenses (Creative Commons, GPL, Other Database, Other), and Tags (described in the next section). /p    pYou can also use the listing to view your own Datasets (Your Datasets), or to look at datasets you have previously bookmarked (Bookmarks). /p    pFinally, a Datasets-specific search bar is available here. This is often the fastest way to find a specific dataset that you are looking for. /p    h4 id=tags-and-tag-pagesTags and Tag Pages/h4    pTags are the most advanced of the searching options available in the Datasets listing page.\\n\\nTags are added by dataset owners to indicate the topic of the Dataset, techniques you can use (e. g. , classification), or the type of the data itself (e. g. , text data). You can navigate to tag pages to browse more content sharing a tag either by clicking on a tag on a Dataset, or by clicking on the Tags dropdown in the site header. /p    pSearching by tags allow you to search for Datasets by topical area. For example, if you are interested in animal shelter data you might try a search with the tag animals; if you are interested in police records a search with crime would do the trick. /p    pTag pages include a section listing the most popular pages with the given tag, making them a great way of searching for datasets by content. /phr width=100% align=left! --Creating a Dataset--h3 id=creating-a-datasetCreating a Dataset/h3pIts easy to create a dataset on Kaggle and doing so is a great way to start a data science portfolio, share reproducible research, or work with collaborators on a project for work or school. You have the option to create private datasets to work solo or with invited collaborators or publish a dataset publicly to Kaggle for anyone to view, download, and analyze. /p    h4 id=navigating-the-dataset-interfaceNavigating the Dataset Interface/h4    pTo publish a private or public dataset, start by navigating to the a href= listing/a. There you will find a New Dataset button. Click on it to open the New Dataset modal.\\n\\n/p    pThe required bare minimum fields for uploading a dataset to Kaggle in descending order are:/p    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpThe strongTitle/strong is the name of the Dataset  e. g.  what will appear in the listing when searching or browsing. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpThe strongURL/strong is the link the Dataset will live at. The slug will first auto-populate and mimic your Title. However, you can hover over the slug to change it right away. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpFinally, you may upload data from one of four sources:/p    \\\\t\\\\tul    \\\\t\\\\t    listrongYour local machine/strong - upload files/folders via drag and drop or by selecting them in your file browser. To speed up file/folder uploads, try uploading them as a ZIP archive; the contents will be unzipped on our side to make them accessible in Notebooks.\\n\\n/li    \\\\t\\\\t    listrongRemote Files/strong - enter list of public URL(s) which identify files to be imported into dataset/li    \\\\t\\\\t    listrongGithub Repository/strong - enter URL to github repository whose files will be imported into dataset/li    \\\\t\\\\t    listrongNotebook Outputs/strong - use inbuilt search to explore publicly available files produced from Kaggles large repository of public Notebooks/li    \\\\t\\\\t/ul    \\\\t/li    /ul    pTo make your dataset more useful for your collaborators and the community it is recommended you update the following settings:     /p    ul style=list-style-type:disc        li    \\\\t\\\\tpThe Sharing menu controls the Datasets visibility. Datasets may be Private (visible only to you and your collaborators, and to Kaggle for purposes consistent with the Kaggle Privacy Policy) or Public (visible to everyone). The default setting is Private. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpThe Licence is the license the dataset is released under (relevant for public datasets). If the license you need doesnt appear in the dropdown, select the Other (specified in description) option and be sure to provide information on the license when writing the dataset description (in the next step). Below is a list of common licenses. \\\\t\\\\t/p    \\\\t\\\\th6 id=common-licensesCommon Licenses/h6            ul    \\\\t\\\\t    li                    strongCreative Commons/strong                     ul                        lia href= Public Domain/a/li                        lia href= BY-NC-SA 4.\\n\\n0/a/li                        lia href= BY-SA 4. 0/a/li                        lia href= BY-SA 3. 0/a/li                        lia href= BY 4. 0 (Attribution 4. 0 International)/a/li                        lia href= BY-NC 4. 0 (Attribution-NonCommercial 4. 0 International)/a/li                        lia href= BY 3. 0 (Attribution 3. 0 Unported)/a/li                        lia href= BY 3. 0 IGO (Attribution 3. 0 IGO)/a/li                        lia href= BY-NC-SA 3. 0 IGO (Attribution-NonCommercial-ShareAlike 3. 0 IGO)/a/li                        lia href= BY-ND 4. 0 (Attribution-NoDerivatives 4. 0 International)/a/li                        lia href= BY-NC-ND 4. 0 (Attribution-NonCommercial-NoDerivatives 4. 0 International)/a/li                    /ul                /li                li                    strongGPL/strong                     ul                        lia href= 2/a/li                        lia href= 3. 0 (GNU Lesser General Public License 3. 0)/a/li                        lia href= 3. 0 (GNU Affero General Public License 3. 0)/a/li                        lia href= 1. 3 (GNU Free Documentation License 1.\\n\\n3)/a/li                    /ul                /li                li                    strongOpen Data Commons/strong                     ul                        lia href= Open Database, Contents: Database Contents/a/li                        lia href= Open Database, Contents:  Original Authors/a/li                        lia href= (ODC Public Domain Dedication and Licence)/a/li                        lia href= 1. 0 (ODC Attribution License)/a/li                    /ul                /li                li                    strongCommunity Data License/strong                     ul                        lia href= Data License Agreement - Permissive - Version 1. 0/a/li                        lia href= Data License Agreement - Sharing - Version 1. 0/a/li                    /ul                /li                li                    strongSpecial/strong                     ul                        lia href= Bank Dataset Terms of Use/a/li                        lia href= API Terms/a/li                        lia href= Government Works/a/li                        lia href= ODP Legal Notice/a/li                    /ul                /li    \\\\t\\\\t/ul        /li    \\\\tli    \\\\t\\\\tpOwner allows you to specify the dataset Owner if you belong to any Organizations. You may assign ownership to yourself or to any Organizations you are a member of (see the section Creating and using organizations to learn more about this feature).\\n\\n/p    \\\\t/li    /ul    pOnce you have provided the required information alongside your data source, click on Create Dataset and your dataset will start processing. Once the dataset is finished processing, you will be taken to your new datasets home page. /p    pNote that if your dataset is very large (multiple gigabytes in size), processing may take a while, up to several minutes. Feel free to navigate away from the browser window whilst processing is inflight as it will continue in the background. /p    pYour datasets has now been created! However, for truly great Datasets, the work doesnt stop there. Once you have specified the required fields there are a few other things you should do in order to maximize your datasets usefulness to the community or your collaborators:/p    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpUpload a cover image. We recommend using a href= for shareable, high resolution images. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpAdd a subtitle to the dataset. This is a short bit of text explaining in slightly more detail what is in it. This subtitle will appear alongside the title in the search listings. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpAdd tags. Tags help users find datasets on topics they are interested in by making them easier to find. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpAdd a description. The description should explain what the dataset is about in long-form text. A great description is extremely useful to Kaggle community members looking to get started with your data.\\n\\n/p    \\\\t/li    \\\\tli    \\\\t\\\\tpPublish a public Notebook. Use Notebooks to show community members or your collaborators how to get started with the data. This can be something simple like an exploratory data analysis or a more complex project reproducing research using the data. /p    \\\\t/li    /ul    pA few examples of well-formatted datasets are a href= Competitive Matchmaking Data/a, a href= Dataset/a, a href= million UK traffic accidents/a, and a href= MNIST/a. /p    h4 id=creating-datasets-from-various-connectorsCreating Datasets from Various Connectors/h4    pAs outlined above, in addition to uploading files from your local machine, you can also create Datasets from various data sources including GitHub, remote URLs (any public file hosted on the web), and Notebook output files. These are each icons that can be found in the Dataset Upload Modal sidebar. /p        h5GitHub and Remote File Datasets/h5        pDatasets created from a GitHub repository or hosted (remote) files are downloaded directly from the remote server to Kaggles cloud storage and, therefore, will consume none of your local networks bandwidth. This makes the remote files connector a convenient solution for creating datasets from large files. /p        pWhen a dataset is created from a github repository or hosted file, the publisher is able to set up automatic interval updates from the datasets Settings tab. Heres an example a href= market dataset/a that updates daily. pDont want to wait for a refresh?\\n\\nNo problem! Click the Update button within the ...  dropdown in the dataset menu header to sync your dataset immediately. /p        h5Notebook Output File Datasets/h5        pCreating a dataset from a Notebooks output files will let you create reproducible data pipelines. To create a dataset from a Notebooks output files, click on the icon in the uploader and search for your Notebook. Alternatively, you can click Create Dataset from the Output tab on your rendered Notebook. Then, select the files you want to use in your dataset. /p        h5Limitations/h5        pIts worth noting that for user experience and technical simplicity, a dataset can be created and versioned from exclusively one data source. That is, data sources currently can not be mixed and matched in any given dataset (for example, a dataset created from a GitHub repository cant also include files uploaded from your local machine). If you would like to use various different data sources in a Notebook you can create multiple datasets and add them both to said Notebook. pThe usual technical specifications for dataset creation apply to connectors too. See the a href= Specifications/a section for more information. /p    h4 id=Updating Dataset using JSON ConfigUpdating Dataset Using JSON Config/h4    pFor advanced users, you may find it easier to update key parameters of your dataset by specifying the details as JSON configuration.\\n\\nTo do this, navigate to your dataset and click Settings, followed by  JSON Config in the menu of options on the left. /p    pYou can update any of the settings you would normally edit through the datasets user interface, such as title, collaborators, licenses, keywords and more. For a reference to the schema you can use for updating dataset settings, you can look at our a href=  documentation/a for the relevant actions within the Public API. /ppPlease note, there are some subtle differences between the Public API schema and the schema supported in the JSON Config settings UI. They are as follows: /p    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpstrongid/strong is omitted as it cannot be changed after dataset creation/p    \\\\t/li    \\\\tli    \\\\t\\\\tp strongresources/strong is omitted as you cannot change the uploaded files using this UI /p    \\\\t/li    \\\\tli    \\\\t\\\\tpThe strongisPrivate/strong is an added boolean option that allows users to change the privacy of their datasets (note: public datasets can NOT be made private)/p    \\\\t/li    \\\\tli    \\\\t\\\\tpstrongcollaborators/strong is an added array of objects with shape code{ username: string; role: read | write}/code that can be used to specify dataset collaborators/p    \\\\t/li    /ulhr width=100% align=left! --Collaborating on Datasets--h3 id=collaborating-on-datasetsCollaborating on Datasets/h3pDataset collaboration is a powerful feature. It allows multiple users to co-own and co-maintain a private or publicly shared dataset.\\n\\nFor example, you can invite collaborators to view and edit a private dataset to work together on preparing it before changing its visibility to public. /ppWhen uploading a Dataset you may choose either yourself or any Organization you are a part of as the Owner of that Dataset. If you select yourself, that Dataset will be created with yourself as the Owner. If you select an Organization, that Organization will be the Owner of the dataset, and every other user in the Organization (including yourself) will be added as a Collaborator with editing privileges (if you are unfamiliar with Organizations, you may also want to read the section Creating and using organizations). /ppThis means that Organizations are an easy way to manage access to datasets or groups of datasets. /p    h4 id=inviting-collaboratorsInviting Collaborators/h4    pAlternatively, you may manage Collaborators directly. To do so, go to any dataset you own and navigate to Settings gt; Sharing. There, use the search box to find and add other users as Dataset collaborators. /p    pIf your Dataset is private, you may choose between giving Collaborators either viewing privileges (Can view) or editing privileges (Can edit). If your Dataset is public, Collaborators can only be added with editing privileges (Can edit), as anyone can view it already. /p    pWhen you add a collaborator, they will receive a notification via email.\\n\\n/p    pa href= Science for Good: Kiva Crowdfunding/a is a great example of a Collaborative Dataset. /p    h4 id=using-notebooks-with-dataset-collaboratorsUsing Notebooks with Dataset Collaborators/h4    pUsing Notebooks, Kaggles interactive code editing and execution environment, is a powerful way to work with your collaborators on a Dataset. You might want to work with collaborators to write public Notebooks that help familiarize other users with your dataset. Or you may want to keep all of your code private among your collaborators as you work on privately shared projects together. /p    pNotebooks you create are private by default, and their sharing settings are distinct from the sharing settings on your Dataset. That is, your Dataset collaborators wont automatically see your private Notebooks. Heres what that means and how you can productively use sharing settings on Datasets and Notebooks together:/p    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpYou can make public Notebooks on a private Dataset which will allow anyone to view your Notebook, but not the underlying private data source. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpIf you want to add view or edit collaborators to a private Notebook (whether the dataset is private or public), you can do so by adding users via Options gt; Sharing on the Notebook. /p    \\\\t/li    /ulhr width=100% align=left!\\n\\n--Resources for Starting a Data Project--h3 id=resources-for-starting-a-data-projectResources for Starting a Data Project/h3pThere are many resources available online to help you get started working on your open data project. /p    h4 id=using-datasetsUsing Datasets/h4    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpa href= Started on Kaggle video tutorials/a: Just started on Kaggle? Not sure what is where and why? Here are our very own Kaggle team tutorials to orient you quickly on navigating the Kaggle platform and creating your own datasets and Notebooks/p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= Guide to Open Data Publishing/a: This article includes the key ingredients to an open data project. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= scraping data in Python/a: A tutorial showing you how to scrape data with BeautifulSoup. It goes over the same code used to create the a href= Beers dataset/a published on Kaggle. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= Kaggle the Home of Open Data/a: Bens post shares instructions for publishing your open data project on Kaggle and how you can explore others datasets. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= an Organization/a: If youre publishing data from an organization, you can create an organization profile first. Then you just select the organization profile from the dropdown near your avatar when publishing (a href=    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= Data Spotlights/a: This series highlights some of the best open data projects on Kaggle.\\n\\n/p    \\\\t/li    \\\\tli    \\\\t\\\\tpHave requests or want to discuss data collection, cleaning, or other aspects of open data projects? Post away in the a href= Discussion forum/a on Kaggle. /p    \\\\t/li    /ul    h4 id=using-notebooksUsing Notebooks/h4    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpa href= Started on Kaggle video tutorials/a: Just started on Kaggle? Not sure what is where and why? Here are our very own Kaggle team tutorials to orient you quickly on navigating the Kaggle platform and creating your own datasets and Notebooks/p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= Learn/a is a great place to start getting hands on with data science and machine learning techniques using Notebooks. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= open data make you happy? An introduction to Kaggle Notebooks/a: Learn how to use Notebooks to explore any combination of datasets published on Kaggle. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= Ways to Map Data in Notebooks/a: A collection of mini-tutorials by Kaggle users for Python and R users. /p    \\\\t/li    /ul    h4 id=analysisAnalysis/h4    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpa href= to Get Started with Data Science in Containers/a: One of our data scientists, Jamie Hall, explains how and why Docker containers are at the heart of Notebooks  reproducible analysis. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href= (Almost) Any Machine Learning Problem by Kaggle Grandmaster Abhishek Thakur/a: Exactly what it says  a great tutorial.\\n\\n/p    \\\\t/li    /ul    h4 id=otherOther/h4    ul style=list-style-type:disc    \\\\tli    \\\\t\\\\tpa href= Datasets Twitter/a: The new account features newly featured datasets plus open data news. /p    \\\\t/li    \\\\tli    \\\\t\\\\tpa href=  Using Open Data/a: A blog by Kaggler MLWave recommended by Triskelion. /p    \\\\t/li    /ulhr width=100% align=left! --Technical Specifications--h3 id=technical-specificationsTechnical Specifications/h3pKaggle Datasets allows you to publish and share datasets privately or publicly. We provide resources for storing and processing datasets, but there are certain technical specifications:/pul style=list-style-type:disc\\\\tli\\\\t\\\\tp100GB per dataset limit/p\\\\t/li\\\\tli\\\\t\\\\tp100GB max private datasets (if you exceed this, either make your datasets public or delete unused datasets)/p\\\\t/li\\\\tli\\\\t\\\\tpA max of 50 top-level files (if you have more, use a directory structure and upload an archive)/p\\\\t/li/ulpWhen you upload a dataset we apply certain processing steps to make the dataset more usable.\\n\\n/pul style=list-style-type:disc\\\\tli\\\\t\\\\tpA complete archive is created so the dataset can be easily downloaded later/p\\\\t/li\\\\tli\\\\t\\\\tpAny archives (e. g. , ZIP files) that you upload are uncompressed so that the files are easily accessible in Notebooks (directory structure is preserved)/p\\\\t/li\\\\tli\\\\t\\\\tpData types for tabular data files are automatically detected (e. g. , geospatial types)/p\\\\t/li\\\\tli\\\\t\\\\tpColumn-level metrics are calculated for tabular data which are viewable on the data explorer on the datasets Data tab/p\\\\t/li/ulpWhen publishing datasets, you might also want to consider a href= technical specifications of Notebooks/a if you intend to use (or encourage other Kaggle users to use) Notebooks to analyze the data. /p\",\"id\":33661,\"isPublished\":true,\"url\":\"docs/datasets\",\"isDocPage\":true,\"allowUnsanitizedHtml\":true});performance && performance. mark && performance. mark(\"CmsPageContainer. componentCouldBootstrap\"); </script>', metadata={'source': '/content/DE_CW/datasets.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the documents are loaded, they are then split into smaller, more manageable chunks. This is done using the split_text_to_chunks(documents) function, which utilizes the RecursiveCharacterTextSplitter() class.\n",
        "\n",
        "The parameters for the text splitter are set as follows:\n",
        "1. chunk_size=1000: This specifies the maximum size of each chunk. In this case, each chunk will contain up to 1000 characters.\n",
        "\n",
        "2. chunk_overlap=50: This is the number of characters that will overlap between adjacent chunks. This overlap can help ensure that no important information is lost at the boundaries between chunks.\n",
        "\n",
        "3. length_function=len: This specifies the function used to calculate the length of the text. In this case, the built-in len function is used, which returns the number of characters in the text.\n",
        "\n",
        "3. add_start_index=True: This indicates that the start index of each chunk (relative to the original document) should be included in the metadata for each chunk.\n",
        "\n",
        "The split_text_to_chunks(documents) function splits each document into chunks and then prints the number of chunks created. It then returns a list of these chunks."
      ],
      "metadata": {
        "id": "4LLSR32Org72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into chunks based on relevance\n",
        "def split_text_to_chunks(documents):\n",
        "\n",
        "    \"\"\"\n",
        "    Split documents into chunks based on recursive character text splitter.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents to be split into chunks.\n",
        "\n",
        "    Returns:\n",
        "        list: List of chunks containing the split content of the documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
        "                                                   chunk_overlap = 50,\n",
        "                                                   length_function = len,\n",
        "                                                   add_start_index = True)\n",
        "\n",
        "    # Split documents into chunks using the initialized text splitter\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Print the number of documents and chunks for verification\n",
        "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "    # Select a chunk at random (in this case, the 11th chunk) for inspection\n",
        "    document = chunks[10]\n",
        "\n",
        "    # Print the content and metadata mentioning the source and the start index of the selected chunk for verification\n",
        "    print(document.page_content)\n",
        "    print(document.metadata)\n",
        "\n",
        "    # Return the list of chunks for further processing\n",
        "    return chunks"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:21.028982Z",
          "iopub.execute_input": "2024-03-30T07:59:21.02961Z",
          "iopub.status.idle": "2024-03-30T07:59:21.035345Z",
          "shell.execute_reply.started": "2024-03-30T07:59:21.029576Z",
          "shell.execute_reply": "2024-03-30T07:59:21.034409Z"
        },
        "trusted": true,
        "id": "yeirmMZy8V2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split documents into chunks based on relevance\n",
        "chunks = split_text_to_chunks(documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-30T07:59:22.815617Z",
          "iopub.execute_input": "2024-03-30T07:59:22.816431Z",
          "iopub.status.idle": "2024-03-30T07:59:23.273285Z",
          "shell.execute_reply.started": "2024-03-30T07:59:22.816396Z",
          "shell.execute_reply": "2024-03-30T07:59:23.272238Z"
        },
        "trusted": true,
        "id": "NFGtZiEE8V2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36be3674-c611-4310-ba2e-1151b5193e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 4 documents into 156 chunks.\n",
            "--Searching for Datasets--h3 id=searching-for-datasetsSearching for Datasets/h3pDatasets is not just a simple data repository. Each dataset is a community where you can discuss data, discover public code and techniques, and create your own projects in Notebooks. You can find many different interesting datasets of all shapes and sizes if you take the time to look around and find them! /ppThe latest and greatest from Datasets is surfaced on Kaggle in several different places. /p    h4 id=newsfeedNewsfeed/h4    pWhen youre logged into your Kaggle account, the a href= homepage/a provides a live newsfeed of what people are doing on the platform. New Datasets uploaded by people you follow and hot Datasets with lots of activity will show up here. By browsing down the page you can check out all the latest updates from your fellow Kagglers. /p    pYou can tweak your news feed to your liking by following other Kagglers. To follow someone, go to their profile page and click on Follow User.\n",
            "{'source': '/content/DE_CW/datasets.txt', 'start_index': 7272}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Based on the parameters set for the text splitter, the 4 documents are split into 156 chunks. This means that the original documents have been broken down into 156 smaller pieces of text, each containing up to 1000 characters, ready for further processing or analysis."
      ],
      "metadata": {
        "id": "nmuTvzADsndP"
      }
    }
  ]
}